---
title: "Text analysis of tweets"
output: html_notebook
---

```{r}
library(rtweet)
library(tidyverse)
library(tidytext)
library(DT)
library(plotly)
library(wordcloud2)
```


Newspapers like the New York Times have twitter accounts that tweet mainly headlines and descriptions of their articles. Examining these gives us a sense of what American news is covering.

You shouldn't need to re-enter your rtweet credentials, but if you do, go back to that notebook and follow those instructions, copy-pasting the passwords here if necessary, and then be sure to delete them again.

```{r}
news_tweets <- get_timeline("nytimes", n = 5000)

```


The following unnests the text, makes it all lowercase, removes punctuation, and calls it news_words:

```{r}
news_words <- news_tweets %>% 
  unnest_tokens(word, text) %>% 
  select(screen_name, word)

```


## Word counts and word cloud

Get a look at the most important words by piping news_words into count(word, sort = T) below:







But that has a lot of simple and unimportant words at the top of the list. Let's get rid of stop words first. This uses anti_join() to remove the stopwords from our words:

```{r}
news_words %>% 
  anti_join(get_stopwords()) %>% 
  count(word, sort = T)
```


Because tweets use web addresses, we sometimes get strange "words" like https. Let's get rid of any of those that make it to the top. 

1. Copy-paste the above chunk.  
2. Add the following line between the anti_join() line and the count() line:
filter(!word == "https")
Look for other weird words like that in first 200 words or so, and add a line for each one you want to remove.








When you get that working, create a word cloud.

1. Add the line top_n(200) to limit the number of words in the cloud to 200
2. add the line wordcloud2(size = .5)















## Sentiment analysis

Sentiment is another word for opinion or emotion. Sentiment analysis is the analysis of text for the opinions or emotions it contains. It is often used in marketing research to see customers' opinions in the reviews they leave on sites like Amazon.

Sentiment analysis uses dictionaries that contain words related to sentiments, or emotions, and then compares the dictionary to the text being analyzed. Let's look at one of the sentiment dictionaries, called bing after one of the researchers who developed it.

```{r}
bing <- get_sentiments("bing")
bing
```



We need to join the sentiment dictionary with our text. The inner_join() command will only keep the words that are in common to both our list of words and the sentiment words.

```{r}
news_words %>% 
  inner_join(bing) %>% 
  count(word, sentiment, sort = TRUE)

```



We can also adjust our sentiment dictionary for our particular needs. For example, 'trump' is probably not a positive word here, it's a proper name, so filter it out by adding the following line above the count() line: 
filter(!word == "trump")








The following code creates a graph of the top words that contribute to each sentiment. Take the code from the previous chunk and put it at the top of the following code to make it run:

```{r}


  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(vars(sentiment), scales = "free") +
  labs(y = "News headlines: Words that contribute the most to each sentiment",
       x = NULL) +
  coord_flip() +
  theme_minimal()
```



Another sentiment dictionary is nrc, which stands for National Research Council, a government agency in Canada similar to the National Science Foundation in the US.


```{r}
nrc <- get_sentiments("nrc")
nrc
```



Notice that this dictionary has several different sentiments, not just positive and negative. 

You can view all the different sentiments by piping nrc into distinct(sentiment). Do that below:










Using the same procedure as above, replicate the graph of words contributing to each sentiment, but with the nrc lexicon. In the top_n() function in the graph, try a smaller number than 10, like 3 or 5.













### Bigrams

So far we have been examining individual words, but it is often useful to examine words that appear together. To prepare the data we use unnest_tokens() again, but now we add token = "ngrams", n = 2 to look at word pairs or bigrams. Here is how to do that:

```{r}

news_tweets %>%
  select(text) %>%                                                 # this selects just the text of the tweets
  unnest_tokens(words, text, token = "ngrams", n = 2)

```


Now count the bigrams by piping the above code into count(words, sort = T).






We have the same problem as before, where most of the top bigrams are unimportant words. We can filter the stopwords, but it's a little more complex. Here's how to do it:

```{r}
news_tweets %>%
  select(text) %>%                                                 # this selects just the text of the tweets
  unnest_tokens(words, text, token = "ngrams", n = 2) %>% 
  separate(words, c("word1", "word2"), sep = " ") %>%          # separate them temporarily
  filter(!word1 %in% stop_words$word) %>%                      # remove if first word is a stop word
  filter(!word2 %in% stop_words$word) %>%                      # remove if second word is a stop word   
  unite(words, word1, word2, sep = " ")                        # put them back together

```







There are still some weird web address words in there. We can get rid of them by creating a vector of the words and then filtering those out the same way. The following code does two new things:  
1. creates a new vector called remove_words with all of the special words we want to remove,  
2. adds new filter lines to check for and remove any words that appear in remove_words  

```{r}
remove_words = c("https", "t.co")

news_tweets %>%
  select(text) %>%                                                 
  unnest_tokens(words, text, token = "ngrams", n = 2) %>% 
  separate(words, c("word1", "word2"), sep = " ") %>%          # separate them temporarily
  filter(!word1 %in% stop_words$word) %>%                      # remove if first word is a stop word
  filter(!word2 %in% stop_words$word) %>%                      # remove if second word is a stop word   
  filter(!word1 %in% remove_words) %>%                         # these two lines remove our remove_words
  filter(!word2 %in% remove_words) %>%                         
  unite(words, word1, word2, sep = " ")                        # put them back together

```


This is a common task in text analysis, creating a dictionary of words and comparing it to your text.

Once you are satisfied that it is working, make a dataframe of this new set of words called news_bigrams. Here's a new way to do that. We have used this as an assignment operator: <- but you can also use it the other direction -> .

Copy-paste the above code chunk, but then put the following line at the end of the last line:
-> news_bigrams

Then view the new data frame by typing in its name.









Now count the top bigrams by piping news_bigrams to count(words, sort = T)








Create a word cloud of these bigrams by copy-pasting the chunk you created above and then adding the following lines: top_n(100), and wordcloud2(size = .5).










What is really useful about bigrams is that it allows us to look for a specific words and see what other words are associated with it. For example, we might be interested in which words follow trump and pelosi. That will give us a sense of how the news describes these two people.


```{r}
first_word <- c("trump", "pelosi")                                  # these need to be lowercase

news_bigrams %>%             
  count(words, sort = TRUE) %>%
  separate(words, c("word1", "word2"), sep = " ") %>%       # separate the two words
  filter(word1 %in% first_word) %>%                          # find first words from our list
  count(word1, word2, wt = n, sort = TRUE) %>% 
  rename(total = nn)

```


Take all of the above chunk that creates the trump/pelosi counts, put it on top of the following chunk (don't forget the pipe to connect them), and run it to create a graph.


```{r}




  mutate(word2 = factor(word2, levels = rev(unique(word2)))) %>%     # put the words in order
  group_by(word1) %>% 
  top_n(5) %>% 
  ggplot(aes(word2, total, fill = word1)) +                          #
  scale_fill_viridis_d() +                                           # set the color palette
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = NULL, title = "Word following:") +
  facet_wrap(~word1, scales = "free") +
  coord_flip()

```













Assignment:
Select another news organization and grab 5000 of their tweets.
Some possibilities:
For Billings Gazette headlines: billingsgazette
Wall Street Journal: wsj
Washington Post: washingtonpost
CNN: cnn
CNN breaking news: cnnbrk
USA Today: usatoday

1. Unnest the words of the tweets, remove stop words and weird web "words", and create a table and a word cloud of the top words.  
2. Conduct a sentiment analysis using bing, remove any errors like trump = positive, and create a graph of the words that contribute most to each sentiment.  
3. Do the same as above but with the nrc sentiment lexicon.  
4. Unnest the tweets as bigrams, remove stop words and errors, and create a table and word cloud of the most common bigrams.  
5. Using the bigrams, look for the most common words that follow two different words. You may choose trump and pelosi, or choose your own.  



